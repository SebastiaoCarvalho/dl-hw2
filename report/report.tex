\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{bm}
\usepackage[top=2cm]{geometry}
\renewcommand{\thesubsubsection}{\thesubsection\ \alph{subsubsection})}

\title{Deep Learning - Homework 2}
\author{99222 - Frederico Silva, 99326 - Sebasti√£o Carvalho}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\section{Question 1}

\subsection{Question 1.1}

Using a single attention head, the output $Z$ is given by:

\begin{equation}
    Z = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
\end{equation}

The complexity of computing this $Z$ is the complexity of computing all the matrix multiplications and the softmax.

Starting with the dimensions of each matrix, we have:
$
    Q \in \mathbb{R}^{L \times D}, K \in \mathbb{R}^{L \times D}, V \in \mathbb{R}^{L \times D}
$

\bigskip

The complexity of computing a matrix product $AB$ where $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times p}$ is $O(mnp)$.
We can prove this since to compute the element $c_{ij}$ of the matrix $C = AB$ we need to compute the dot product of the $i$-th row of 
$A$ with the $j$-th column of $B$, and this dot product has complexity $O(n)$, because it is the sum of $n$ products of real numbers.
Since we need to compute $mn$ elements of $C$, the complexity of computing $C$ is $O(mnp)$.

\bigskip

Let's consider $P = softmax \left( \frac{QK^T}{\sqrt{d_k}} \right)$. With this $P$ we can compute $Z = PV$.

In our case, we first need to compute $QK^T$ and since $Q \in \mathbb{R}^{L \times D}$ and $K^T \in \mathbb{R}^{D \times L}$, 
the complexity of this operation is $O(L^2D)$.

Then, we need to divide each element of $QK^T$ by $\sqrt{d_k}$, which has complexity $O(L^2)$.
Finally, we need to compute the softmax of each row of the matrix $QK^T / \sqrt{d_k}$, which has complexity $O(L^2)$.
The complexity of computing $P$ is $O(L^2D + L^2 + L^2) = O(L^2D)$. 
Since $P = softmax \left( \frac{QK^T}{\sqrt{d_k}} \right)$, $P \in \mathbb{R}^{L \times L}$.

\bigskip

Lastly we need to compute the matrix product $PV$. Since $P \in \mathbb{R}^{L \times L}$ and $V \in \mathbb{R}^{L \times D}$,
the complexity of this operation is $O(L^2D)$.

With this the final complexity of computing $Z$ is $O(L^2D)$, since the complexity of computing $P$ is $O(L^2D)$ 
and the complexity of computing $PV$ is $O(L^2D)$.

\bigskip

Let's consider the numebr of hidden units (D) is fixed, so the complexity of computing $Z$ is $O(L^2)$.

This may cause a problem for long sequences of text, since the complexity of computing $Z$ is $O(L^2)$, where $L$ is the length of the sequence.
This means that the complexity of computing $Z$ is quadratic in the length of the sequence, which is not good for long sequence inputs.

\subsection{Question 1.2}

\subsection{Question 1.3}

\subsection{Question 1.4}

\section{Question 2}

\subsection{Question 2.1}
After running the code, the best configuration was for the learning rate of 0.01.
The following plots were generated:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/CNN-training-loss-0.01-0.7-0-sgd-False.png}
    \caption{Training loss for $\eta=0.01$}
    \label{fig:2.1-training_loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/CNN-validation-accuracy-0.01-0.7-0-sgd-False.png}
    \caption{Validation accuracy for $\eta=0.01$}
    \label{fig:2.1-validation_accuracy}
\end{figure}

The final test accuracy was 0.8280.

\subsection{Question 2.2}
The performance of this network was slightly worse than the previous one, 
having achieved a final test accuracy of 0.8147.

\subsection{Question 2.3}
Both network present the same number of parameters, 224892.
The difference in performance between the two networks, resides in the use 
of max pooling layers. Max pooling can help the network focusing on the most
important features, making the network more robust to small changes in the
input. Furthermore, max pooling can also help with overfitting. In our case, 
the use of max pooling layers helped the network to achieve a better test 
accuracy results.

\section{Question 3}

\subsection{Question 3.1}

\subsection{Question 3.2}

\subsection{Question 3.3}

\subsection{Question 3.4}

\section{Credits}

\section{Sources}

\begin{itemize}
    \item \href{https://www.educative.io/answers/what-is-a-max-pooling-layer-in-cnn}{What is a max pooling layer in CNN?}
\end{itemize}

\end{document}